{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "url = 'https://health.usnews.com/doctors/new-jersey'\n",
    "#data = data.encode('utf-8')\n",
    "\n",
    "headers = {}\n",
    "headers['User-Agent'] = \"Mozilla/5.0 (X11; Linux i686)\"\n",
    "\n",
    "req = urllib.request.Request(url, headers=headers)\n",
    "resp = urllib.request.urlopen(req)\n",
    "resp_data = resp.read()\n",
    "\n",
    "#print(resp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp_data, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = soup.findAll('a', {'class': 'search-result-link bar-tighter'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for do in doc:\n",
    "    #print ('https://health.usnews.com' + do.get('href', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = ['https://health.usnews.com' + do.get('href', None) for do in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import defaultdict\n",
    "#data=defaultdict(list)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for link in links:\n",
    "    #headers = {}\n",
    "    #headers['User-Agent'] = \"Mozilla/5.0 (X11; Linux i686)\"\n",
    "    #doc_req = urllib.request.Request(link,headers=headers)\n",
    "    #doc_resp = urllib.request.urlopen(doc_req)\n",
    "    #doc_resp_data = doc_resp.read()\n",
    "    #doc_soup = BeautifulSoup(doc_resp_data, 'html.parser')\n",
    "    #doc_name = doc_soup.find('h1')\n",
    "    #doc_name_text =  (doc_name.text).strip()\n",
    "    #print (doc_name_text)\n",
    "    ######join = \"\".join(line.strip() for line in doc_name_text.split(\"\\n\"))\n",
    "    #doc_overview = doc_soup.find('p')\n",
    "    #doc_overview_text = (doc_overview.text)\n",
    "    #print ('Location :', doc_overview_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for link in links:\n",
    "    #headers = {}\n",
    "    #results = {}\n",
    "    #headers['User-Agent'] = \"Mozilla/5.0 (X11; Linux i686)\"\n",
    "    #doc_req = urllib.request.Request(link,headers=headers)\n",
    "    #doc_resp = urllib.request.urlopen(doc_req)\n",
    "    #doc_resp_data = doc_resp.read()\n",
    "    #doc_soup = BeautifulSoup(doc_resp_data, 'html.parser')\n",
    "    #doc_name = doc_soup.find('h1')\n",
    "    #doc_name_text =  (doc_name.text).strip()\n",
    "    #doc_name_text_mod = (re.sub('\\s+', ' ', doc_name_text))\n",
    "    #print (doc_name_text)\n",
    "    ######join = \"\".join(line.strip() for line in doc_name_text.split(\"\\n\"))\n",
    "    #doc_overview = doc_soup.find('p')\n",
    "    #doc_overview_text = (doc_overview.text).strip()\n",
    "    #doc_overview_text_mod = (re.sub('\\n\\| ', ', ', doc_overview_text))\n",
    "    #print ('Location :', doc_overview_text)\n",
    "    #results[doc_name_text_mod] = doc_overview_text_mod\n",
    "    #print (results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(link):\n",
    "        headers = {}\n",
    "        doctor = []\n",
    "        headers['User-Agent'] = \"Mozilla/5.0 (X11; Linux i686)\"\n",
    "        doc_req = urllib.request.Request(link,headers=headers)\n",
    "        doc_resp = urllib.request.urlopen(doc_req)\n",
    "        doc_resp_data = doc_resp.read()\n",
    "        doc_soup = BeautifulSoup(doc_resp_data, 'html.parser')\n",
    "        doc_name = doc_soup.find('h1')\n",
    "        doc_name_text =  (doc_name.text).strip()\n",
    "        doc_name_text_mod = (re.sub('\\s+', ' ', doc_name_text))\n",
    "        doc_name_text_mod_1  = ('Name' ':' +doc_name_text_mod)\n",
    "        doctor.append(doc_name_text_mod_1)\n",
    "        doc_overview = doc_soup.find('p')\n",
    "        doc_overview_text = (doc_overview.text).strip()\n",
    "        doc_overview_text_mod = (re.sub('\\n\\| ', ', ', doc_overview_text))\n",
    "        doc_overview_text_mod_1  = ('Specialised and Location' ':' + doc_overview_text_mod)\n",
    "        doctor.append(doc_overview_text_mod_1)\n",
    "        #print (doctor)\n",
    "        dicto =  (dict(s.split(':') for s in doctor))\n",
    "        #print(dicto)\n",
    "        return dicto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Dr. Tajwar Aamir', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Bernard Aaron', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Joseph Aaron', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Michael Aaron', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Darryl Aarons', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. William Aarons', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Sirike Aasmaa', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Mario Abad', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Bilal Abadi', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Nelly Abadir', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Esmeralda Abano-Mendoza', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Antonio Abary', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Nicole Abate', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Holly Abate Bersalona', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Muhammad Abbas', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Shahida Abbas', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Arshia Abbasi', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Faheem Abbasi', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Muhammad Abbasi', 'Specialised and Location': 'Health'}\n",
      "{'Name': 'Dr. Muhammad Rashid Abbasi', 'Specialised and Location': 'Health'}\n"
     ]
    }
   ],
   "source": [
    "##SUcccess without multi threading\n",
    "for link in links:\n",
    "    headers = {}\n",
    "    doctor = []\n",
    "    headers['User-Agent'] = \"Mozilla/5.0 (X11; Linux i686)\"\n",
    "    doc_req = urllib.request.Request(link,headers=headers)\n",
    "    doc_resp = urllib.request.urlopen(doc_req)\n",
    "    doc_resp_data = doc_resp.read()\n",
    "    doc_soup = BeautifulSoup(doc_resp_data, 'html.parser')\n",
    "    doc_name = doc_soup.find('h1')\n",
    "    doc_name_text =  (doc_name.text).strip()\n",
    "    doc_name_text_mod = (re.sub('\\s+', ' ', doc_name_text))\n",
    "    doc_name_text_mod_1  = ('Name' ':' +doc_name_text_mod)\n",
    "    doctor.append(doc_name_text_mod_1)\n",
    "    \n",
    "    doc_overview = doc_soup.find('p')\n",
    "    doc_overview_text = (doc_overview.text).strip()\n",
    "    doc_overview_text_mod = (re.sub('\\n\\| ', ', ', doc_overview_text))\n",
    "    doc_overview_text_mod_1  = ('Specialised and Location' ':' + doc_overview_text_mod)\n",
    "    doctor.append(doc_overview_text_mod_1)\n",
    "    #print (doctor)\n",
    "    dicto =  (dict(s.split(':') for s in doctor))\n",
    "    print(dicto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dicto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import os\n",
    "import timeit\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Name\": \"Dr. Tajwar Aamir MD\", \"Specialised and Location\": \"Pediatrics, Princeton, NJ\"}'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Test json not a part of code\n",
    "import json\n",
    "\n",
    "data = {}\n",
    "data['key'] = 'value'\n",
    "json_data = json.dumps(data)\n",
    "my_json_string = json.dumps({'Name': 'Dr. Tajwar Aamir MD', 'Specialised and Location': 'Pediatrics, Princeton, NJ'})\n",
    "\n",
    "my_json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Process ID:  15769\n",
      "26038.884650907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for Cropping is - 16.58150397800273 seconds or 0.2763583996333788 minutes\n",
      "End\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  20 out of  20 | elapsed:   16.6s finished\n"
     ]
    }
   ],
   "source": [
    "#Failed by multi threading\n",
    "#https://joblib.readthedocs.io/en/latest/parallel.html\n",
    "if __name__ == '__main__':\n",
    "    print (\"Start\")\n",
    "    start = timeit.default_timer()\n",
    "    print (\"Process ID: \",os.getpid())\n",
    "    print (start)\n",
    "    links = ['https://health.usnews.com' + do.get('href', None) for do in doc]\n",
    "    Parallel(n_jobs=2, verbose=1)(delayed(extract_data)(link) for link in (links))\n",
    "    extraction_time = timeit.default_timer()\n",
    "    exectime = extraction_time - start\n",
    "    print(\"Time for Cropping is -\", (exectime), \"seconds or\",(exectime)/60, \"minutes\")\n",
    "    print (\"End\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = ['https://health.usnews.com' + do.get('href', None) for do in doc]\n",
    "#(extract_data(link) for link in (links))\n",
    "for link in links:\n",
    "    extract_data(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   37.3s finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    links = ['https://health.usnews.com' + do.get('href', None) for do in doc]\n",
    "    r = Parallel(n_jobs=1, verbose=1)(delayed(extract_data)(link) for link in (links))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'Dr. Tajwar Aamir', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Bernard Aaron', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Joseph Aaron', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Michael Aaron', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Darryl Aarons', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. William Aarons', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Sirike Aasmaa', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Mario Abad', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Bilal Abadi', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Nelly Abadir', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Esmeralda Abano-Mendoza', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Antonio Abary', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Nicole Abate', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Holly Abate Bersalona', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Muhammad Abbas', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Shahida Abbas', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Arshia Abbasi', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Faheem Abbasi', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Muhammad Abbasi', 'Specialised and Location': 'Health'},\n",
       " {'Name': 'Dr. Muhammad Rashid Abbasi', 'Specialised and Location': 'Health'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
